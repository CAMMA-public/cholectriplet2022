{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cZB16RvwSQYD"
   },
   "source": [
    "Copyright &copy; CAMMA, ICube, University of Strasbourg. All Rights Reserved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P8OJJD-kSTcD"
   },
   "source": [
    "<div>\n",
    "<a href=\"https://cholectriplet2022.grand-challenge.org/\">\n",
    "<img src=\"https://rumc-gcorg-p-public.s3.amazonaws.com/b/649/banner2022.x10.jpeg\" align=\"left\"/>\n",
    "</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xu8aMfE5S74v"
   },
   "source": [
    "## <h1><center>Getting Started</center></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ceAz2udUTolO"
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7hLKusP2TpGJ"
   },
   "source": [
    "In this notebook, we provide some sample code to help familiarize yourself with the challenge, the dataset and the metrics. These are minimal examples to help illustrate a simple deep learning pipeline applied on a small subset of the Action Triplet dataset, **CholecT50**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xcQdB8OZWjcb"
   },
   "source": [
    "# Data Loading and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vyjsFvOg3gso"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries for this module\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from glob import glob\n",
    "import json\n",
    "import random\n",
    "from matplotlib import patches\n",
    "\n",
    "\n",
    "print(\"Libraries successfully imported!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P1K8d3LssreJ"
   },
   "source": [
    "Here, we use a small subset of the CholecT50 dataset available at this link: https://seafile.unistra.fr/f/ba1427a82ecc4ce18566/?dl=1. If you are running this notebook on Colab, you can run the cell below to download and unzip the sample dataset to the current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PSeXm7OSm_D3"
   },
   "outputs": [],
   "source": [
    "# Ignore this cell if you have already downloaded and extracted the dataset\n",
    "\n",
    "!wget -O CholecT50_sample.zip https://seafile.unistra.fr/f/ba1427a82ecc4ce18566/?dl=1\n",
    "!unzip -o CholecT50_sample.zip\n",
    "\n",
    "print(\"Dataset successfully extracted!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kvzcRoaNYds7"
   },
   "outputs": [],
   "source": [
    "# Change the dataset_path mentioned below appropriately if you have exracted the data to a different directory \n",
    "\n",
    "dataset_path = './CholecT50_sample/'\n",
    "\n",
    "data_path = os.path.join(dataset_path, 'data')\n",
    "triplet_path = os.path.join(dataset_path, 'triplet')\n",
    "dict_path = os.path.join(dataset_path, 'dict')\n",
    "video_names = os.listdir(data_path)                   \n",
    "\n",
    "print(\"Dataset paths successfully defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gk2y7u2s7i5J"
   },
   "outputs": [],
   "source": [
    "# Create dictionary mapping triplet ids to readable label\n",
    "\n",
    "with open(os.path.join(dict_path, 'triplet.txt'), 'r') as f:\n",
    "  triplet_info = f.readlines()\n",
    "  triplet_dict = {}\n",
    "  for l in triplet_info:\n",
    "    triplet_id, triplet_label = l.split(':')\n",
    "    triplet_dict[int(triplet_id)] = triplet_label.rstrip()\n",
    "\n",
    "print('Random triplet id and its human readable label\\n')\n",
    "random_triplet_id = np.random.choice(list(triplet_dict.keys()))\n",
    "print('Triplet id: ', random_triplet_id, '\\nReadable label: ', triplet_dict[random_triplet_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wLDza6yst9-I"
   },
   "outputs": [],
   "source": [
    "def generator(data_path, triplet_path, video_names, batch_size, shuffle_videos=False):\n",
    "  \"\"\" Defines a simple generator that returns sequential batches of input images and  their \n",
    "      corresponding triplet labels, video names, and frame ids.\n",
    "        Args:\n",
    "          data_path:     Path to directory containing a folder for each video\n",
    "          triplet_path:  Path to folder containing a txt file for each video\n",
    "                         listing the frame id, and binary label for all of the 100 considered \n",
    "                         triplet classes\n",
    "          video_names:   Names of the videos that will be retruned by this generator. These names\n",
    "                         should correpond to a folder in data_path and a txt file in triplet path\n",
    "          batch_size:    Batch size of outputs yielded by the generator\n",
    "          shuffle_videos:To perform a shuffling of videos (Note: frames will be returned sequentially!)  \n",
    "        Returns:\n",
    "          image batch     : Batch of images\n",
    "          triplet batch   : Batch of triplet labels ([N] vectors)\n",
    "          video_name_batch: Batch of video name strings\n",
    "          frame_id_batch  : Batch of integer frame ids\n",
    "    \"\"\"\n",
    "\n",
    "  if shuffle_videos:\n",
    "    video_names = np.random.shuffle(video_names)\n",
    "\n",
    "  image_batch, triplet_batch, video_name_batch, frame_id_batch = [], [], [], []\n",
    "\n",
    "  for video_name in video_names:\n",
    "    with open(os.path.join(triplet_path, video_name + '.txt'), mode='r') as infile:\n",
    "      reader = csv.reader(infile)\n",
    "\n",
    "      for line in reader:\n",
    "        line = np.array(line, np.int64)\n",
    "        frame_id, triplet_label = line[0], line[1:]\n",
    "        image_path = os.path.join(data_path, video_name, \"%06d.png\" % frame_id)\n",
    "        image = np.array(Image.open(image_path), np.float32) / 255.0\n",
    "\n",
    "        image_batch.append(image)\n",
    "        triplet_batch.append(triplet_label)\n",
    "        video_name_batch.append(video_name)\n",
    "        frame_id_batch.append(int(frame_id))\n",
    "\n",
    "        if len(frame_id_batch) == batch_size:\n",
    "          yield image_batch, triplet_batch, video_name_batch, frame_id_batch\n",
    "          image_batch, triplet_batch, video_name_batch, frame_id_batch = [], [], [], []\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h0SZBNWSyf7g"
   },
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "gen = generator(data_path, triplet_path, video_names, batch_size)\n",
    "\n",
    "for images, triplet_labels, video_names, frame_ids in gen:\n",
    "  for batch in range(batch_size):\n",
    "    print('\\nVisualizing image...\\n')\n",
    "    print('Video name: ', video_names[batch], ' Frame_id', frame_ids[batch])\n",
    "    plt.imshow(images[batch])\n",
    "    plt.show()\n",
    "    print('\\nEncoding showing which of the 100 considered action triplets are represented in the image\\n')\n",
    "    print(triplet_labels[batch])\n",
    "    print('\\nReadable labels\\n')\n",
    "    for triplet in np.where(triplet_labels[batch])[0]:\n",
    "      print(triplet_dict[triplet])\n",
    "    \n",
    "  break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fBl9-4gOWof0"
   },
   "source": [
    "#  Building and Running Models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L0rkzEkh4ysO"
   },
   "source": [
    "We build and perform a simple forward pass of an image through a few layer convolutional network to predict the probability of each of the considered triplets being represented in the input image.\n",
    "\n",
    "Note: Please run the cells in the previous module Data Loading and Visualization before running this module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qxDnLbKQ35t7"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries for this module. You can skip ahead if you prefer a PyTorch based example.\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(\"Libraries successfully imported!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2JbM-RYZEJf4"
   },
   "source": [
    "Defining a simple neural network using tf.keras. You can skip ahead if you prefer to use torch.nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vl65rM9IAmqi"
   },
   "outputs": [],
   "source": [
    "# Defining the neural network architecture\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Conv2D(\n",
    "    filters=16, kernel_size=3, strides=2, activation=\"relu\", input_shape=(480, 854, 3))\n",
    ")                 \n",
    "model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides = 2, activation=\"relu\")) \n",
    "model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides = 2, activation=\"relu\")) \n",
    "model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides = 2, activation=\"relu\")) \n",
    "model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides = 2, activation=\"relu\")) \n",
    "model.add(tf.keras.layers.Flatten())                                     \n",
    "model.add(tf.keras.layers.Dense(units=4096, activation=\"relu\"))                 \n",
    "model.add(tf.keras.layers.Dense(units=2048, activation=\"relu\"))            \n",
    "model.add(tf.keras.layers.Dense(units=100, activation=\"sigmoid\"))    \n",
    "\n",
    "print(\"Neural network architecture successfully defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uZ_Yp4e6A_Q-"
   },
   "outputs": [],
   "source": [
    "model.build([1, 480, 854, 3])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gfaJhd471PZJ"
   },
   "source": [
    "Performing a simple forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6aZ4baDLDAYS"
   },
   "outputs": [],
   "source": [
    "input_4d = np.expand_dims(images[0], axis=0)\n",
    "print('Performing a simple forward pass on our untrained network for a test image')\n",
    "plt.imshow(images[0])\n",
    "plt.show()\n",
    "print('\\nPrediction\\n')\n",
    "print(model.predict(input_4d)[0])\n",
    "print('\\nLabel\\n')\n",
    "print(triplet_labels[0])\n",
    "print('\\nReadable label\\n')\n",
    "for triplet_id in np.where(triplet_labels[0])[0]:\n",
    "  print(triplet_dict[triplet_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j2M05KcCQ0eA"
   },
   "source": [
    "[**OPTIONAL**] Using PyTorch to make a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cvjIpgGkRKge"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries for this module.\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.nn import Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yj0iOFBzRnSp"
   },
   "outputs": [],
   "source": [
    "class MyModel(Module):\n",
    "    # define model elements\n",
    "    def __init__(self, h, w):\n",
    "        super(MyModel, self).__init__()\n",
    "        # input to first hidden layer\n",
    "        self.conv1 = nn.Conv2d(3, 32, (3,3))\n",
    "        self.conv2 = nn.Conv2d(32, 64, (3,3))\n",
    "        self.conv3 = nn.Conv2d(64, 128, (3,3))\n",
    "        self.conv4 = nn.Conv2d(128, 256, (3,3))\n",
    "        self.pool1 = nn.MaxPool2d((3,3), stride=(2,2))\n",
    "        self.pool2 = nn.MaxPool2d((2,2), stride=(2,2))\n",
    "        self.pool3 = nn.MaxPool2d((2,2), stride=(2,2))\n",
    "        self.h   = int(h/8 - 4)\n",
    "        self.w   = int(w/8 - 4)\n",
    "        self.mlp = nn.Linear(self.h*self.w*256, 100)        \n",
    "        self.act1 = nn.ReLU()\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.act3 = nn.ReLU()        \n",
    "        self.act4 = nn.Sigmoid()\n",
    "    # forward propagate input\n",
    "    def forward(self, X):\n",
    "        # input to first hidden layer\n",
    "        X = self.conv1(X)\n",
    "        X = self.act1(X)\n",
    "        X = self.pool1(X)\n",
    "        # second hidden layer\n",
    "        X = self.conv2(X)\n",
    "        X = self.act2(X)\n",
    "        X = self.pool2(X)\n",
    "        # second hidden layer\n",
    "        X = self.conv3(X)\n",
    "        X = self.act3(X)\n",
    "        X = self.pool3(X)\n",
    "        # second hidden layer\n",
    "        X = self.conv4(X)\n",
    "        # flatten\n",
    "        X = X.view(-1, self.h*self.w*256)\n",
    "        # output layer\n",
    "        X = self.mlp(X)\n",
    "        X = self.act4(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U4z0Z9rcSMMd"
   },
   "outputs": [],
   "source": [
    "input_4d = np.expand_dims(images[0], axis=0)\n",
    "# Converting to Channel first. NHWC --> NCHW\n",
    "input_4d = np.transpose(input_4d, [0, 3, 1, 2])\n",
    "input_4d = torch.from_numpy(input_4d)\n",
    "print('Performing a simple forward pass on our untrained network for a test image')\n",
    "plt.imshow(images[0])\n",
    "plt.show()\n",
    "print('\\nPrediction\\n')\n",
    "model  = MyModel(480, 854)\n",
    "print(model(input_4d)[0])\n",
    "print('\\nLabel\\n')\n",
    "print(triplet_labels[0])\n",
    "print('\\nReadable label\\n')\n",
    "for triplet_id in np.where(triplet_labels[0])[0]:\n",
    "  print(triplet_dict[triplet_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U_4F2W7uWs5V"
   },
   "source": [
    "#  Spatial detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore if you have already downloaded and extracted the spatial detection samples\n",
    "\n",
    "!wget -O sample-spatial.zip https://seafile.unistra.fr/f/912c37e649a249aaa604/?dl=1\n",
    "!unzip -o sample-spatial.zip\n",
    "\n",
    "print(\"Dataset successfully extracted!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading images, annotations, instrument lookup table\n",
    "\n",
    "imgs = sorted(glob(\"sample-spatial/video/*.png\"))\n",
    "img_dict = {\n",
    "  int(os.path.basename(path).split(\".\")[0]): path\n",
    "  for path in imgs\n",
    "}\n",
    "\n",
    "with open(\"sample-spatial/label.json\") as f:\n",
    "  annotations = {\n",
    "    int(k): v\n",
    "    for k, v in json.load(f).items()\n",
    "  }\n",
    "\n",
    "with open(\"CholecT50_sample/dict/instrument.txt\") as f:\n",
    "  instrument_buf = [\n",
    "    lin.strip().split(\":\") for lin in f.readlines()\n",
    "  ]\n",
    "  instrument_table = {\n",
    "    u[1]: int(u[0])\n",
    "    for u in instrument_buf\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One random detection\n",
    "\n",
    "def one_random_detection():\n",
    "  triplet_id = np.random.randint(0, 100)\n",
    "  triplet_string = triplet_dict[triplet_id]\n",
    "  tool_id = instrument_table[triplet_string.split(\",\")[0]]\n",
    "  tool_prob = np.random.uniform(0, 0.6)\n",
    "  bbox_x, bbox_y = np.random.uniform(0, 0.6, size=2)\n",
    "  bbox_w = np.random.uniform(0.15, 0.4)\n",
    "  bbox_h = np.random.uniform(0.15, 0.4)\n",
    "  res = [\n",
    "    triplet_id,\n",
    "    tool_id,\n",
    "    tool_prob,\n",
    "    bbox_x,\n",
    "    bbox_y,\n",
    "    bbox_w,\n",
    "    bbox_h\n",
    "  ]\n",
    "  return res\n",
    "\n",
    "# Random detections for one image.\n",
    "# This function is a placeholder for an object detector, hence the unused \"img\" argument.\n",
    "\n",
    "def generate_random_detections(img):\n",
    "  n_detect = np.random.randint(1, 3)\n",
    "  detections = []\n",
    "  for _ in range(n_detect):\n",
    "    detections.append(one_random_detection())\n",
    "  return detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_id = random.choice(list(annotations.keys()))\n",
    "\n",
    "im = Image.open(img_dict[img_id])\n",
    "true_w, true_h = im.size\n",
    "fig, ax = plt.subplots(figsize=(16, 9))\n",
    "ax.set_axis_off()\n",
    "ax.imshow(im)\n",
    "\n",
    "im_gt = annotations[img_id]\n",
    "im_pd = generate_random_detections(im)\n",
    "\n",
    "# Visualize ground truth bboxes in green\n",
    "for bbox in im_gt:\n",
    "  ax.add_patch(\n",
    "    patches.Rectangle(\n",
    "      (\n",
    "        bbox[\"instrument\"][2] * true_w,\n",
    "        bbox[\"instrument\"][3] * true_h\n",
    "      ),\n",
    "      bbox[\"instrument\"][4] * true_w,\n",
    "      bbox[\"instrument\"][5] * true_h,\n",
    "      fill=False,\n",
    "      edgecolor=\"green\",\n",
    "      lw=3\n",
    "    )\n",
    "  )\n",
    "  ax.text(\n",
    "    bbox[\"instrument\"][2] * true_w,\n",
    "    bbox[\"instrument\"][3] * true_h + 12,\n",
    "    \"#{}: {}\".format(\n",
    "      bbox[\"triplet\"],\n",
    "      triplet_dict[bbox[\"triplet\"]]\n",
    "    ),\n",
    "    color=\"white\",\n",
    "    fontsize=12,\n",
    "    bbox={\"facecolor\": \"green\", \"alpha\": 0.4}\n",
    "  )\n",
    "\n",
    "# Visualize predicted bboxes in blue\n",
    "for bbox in im_pd:\n",
    "  ax.add_patch(\n",
    "    patches.Rectangle(\n",
    "      (\n",
    "        bbox[3] * true_w,\n",
    "        bbox[4] * true_h\n",
    "      ),\n",
    "      bbox[5] * true_w,\n",
    "      bbox[6] * true_h,\n",
    "      fill=False,\n",
    "      edgecolor=\"blue\",\n",
    "      lw=3\n",
    "    )\n",
    "  )\n",
    "  ax.text(\n",
    "    bbox[3] * true_w,\n",
    "    bbox[4] * true_h + 12,\n",
    "    \"#{}: {} - {:.01f}%\".format(\n",
    "      bbox[0],\n",
    "      triplet_dict[bbox[0]],\n",
    "      100 * bbox[2]\n",
    "    ),\n",
    "    color=\"white\",\n",
    "    fontsize=12,\n",
    "    bbox={\"facecolor\": \"blue\", \"alpha\": 0.4}\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U_4F2W7uWs5V"
   },
   "source": [
    "#  Metrics and Evaluation \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models will be evaluated using the ```ivtmetrics``` module, specifically designed for surgical action triplet recognition and spatial detection. More information available at: [https://github.com/CAMMA-public/ivtmetrics](https://github.com/CAMMA-public/ivtmetrics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation. If already installed you may skip this cell.\n",
    "\n",
    "!pip install ivtmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KLK2JMF4ET2K"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries for this module\n",
    "\n",
    "import numpy as np\n",
    "import ivtmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage. Here we use randomly generated ground truth and predicted values\n",
    "\n",
    "half_0s = np.zeros(shape=[20,100], dtype=np.int64)\n",
    "half_1s = np.ones(shape=[20,100], dtype=np.int64)\n",
    "\n",
    "# Ground truth\n",
    "vid230_gt = np.concatenate((half_0s, half_1s), axis=0)\n",
    "vid231_gt = np.concatenate((half_1s, half_0s), axis=0)\n",
    "\n",
    "# Predictions\n",
    "vid230_pd = np.random.random((40,100))\n",
    "vid231_pd = np.random.random((40,100))\n",
    "\n",
    "videos = [\n",
    "  (vid230_gt, vid230_pd),\n",
    "  (vid231_gt, vid231_pd)\n",
    "]\n",
    "\n",
    "# Initialize metric accumulators\n",
    "recognize = ivtmetrics.Recognition(num_class=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in videos:\n",
    "# loop over videos\n",
    "  for gt, pd in zip(*v):\n",
    "    # loop over batches of frames in the video (batch size 1 here)\n",
    "    batch_gt = np.expand_dims(gt, axis=0)\n",
    "    batch_pd = np.expand_dims(pd, axis=0)\n",
    "    # accumulate results\n",
    "    recognize.update(batch_gt, batch_pd)\n",
    "  # signal end of video to accumulators\n",
    "  recognize.video_end()\n",
    "\n",
    "print(\"Instrument mAP            : {}\".format(\n",
    "  recognize.compute_video_AP(\"i\", ignore_null=True)[\"mAP\"])\n",
    ")\n",
    "print(\"Verb mAP                  : {}\".format(\n",
    "  recognize.compute_video_AP(\"v\", ignore_null=True)[\"mAP\"])\n",
    ")\n",
    "print(\"Target mAP                : {}\".format(\n",
    "  recognize.compute_video_AP(\"t\", ignore_null=True)[\"mAP\"])\n",
    ")\n",
    "print(\"Instrument-Verb mAP       : {}\".format(\n",
    "  recognize.compute_video_AP(\"iv\", ignore_null=True)[\"mAP\"])\n",
    ")\n",
    "print(\"Instrument-Target mAP     : {}\".format(\n",
    "  recognize.compute_video_AP(\"it\", ignore_null=True)[\"mAP\"])\n",
    ")\n",
    "print(\"Instrument-Verb-Target mAP: {}\".format(\n",
    "  recognize.compute_video_AP(\"ivt\", ignore_null=True)[\"mAP\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class AP results are also available\n",
    "\n",
    "recognize.compute_video_AP(\"ivt\", ignore_null=True)[\"AP\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To try ivtmetrics's spatial detection metrics, we reuse the content from the \"Spatial detection\" section.\n",
    "\n",
    "# Correct format for labels\n",
    "def format_labels(label_dict):\n",
    "  res = [\n",
    "    int(label_dict[\"instrument\"][0]),\n",
    "    int(label_dict[\"instrument\"][1]),\n",
    "    1.0,\n",
    "    label_dict[\"instrument\"][2],\n",
    "    label_dict[\"instrument\"][3],\n",
    "    label_dict[\"instrument\"][4],\n",
    "    label_dict[\"instrument\"][5]\n",
    "  ]\n",
    "  return res\n",
    "\n",
    "images = [\n",
    "  Image.open(img_dict[iid])\n",
    "  for iid in annotations.keys()\n",
    "]\n",
    "\n",
    "# Ground truth\n",
    "vid_gt = [\n",
    "  [format_labels(lb) for lb in frame_annotations]\n",
    "  for frame_annotations in annotations.values()\n",
    "]\n",
    "\n",
    "# Predictions\n",
    "vid_pd = [generate_random_detections(im) for im in images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize detection buffers\n",
    "detect = ivtmetrics.Detection(num_class=100)\n",
    "\n",
    "# Only one video available in this example\n",
    "for frame_gt, frame_pd in zip(vid_gt, vid_pd):\n",
    "  # detect.update takes frames in batches (batch size 1 here)\n",
    "  detect.update([frame_gt], [frame_pd])\n",
    "# Signal end of video to accumulators\n",
    "detect.video_end()\n",
    "\n",
    "print(\"Instrument-Verb-Target mAP: {}\".format(\n",
    "  detect.compute_video_AP(\"ivt\")[\"mAP\"])\n",
    ")\n",
    "print(\"Instrument-Verb-Target mean recall: {}\".format(\n",
    "  detect.compute_video_AP(\"ivt\")[\"mRec\"])\n",
    ")\n",
    "print(\"Instrument-Verb-Target mean precision: {}\".format(\n",
    "  detect.compute_video_AP(\"ivt\")[\"mPre\"])\n",
    ")\n",
    "# Since detections are randomly generated in this example, performance is most likely 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O3LZGSrs85K6"
   },
   "source": [
    "#  Saving Results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_e5-i9jt88WW"
   },
   "source": [
    "A minimal working example to save your model results using a docker image is provided here: https://seafile.unistra.fr/f/a495966e56e84bf0a834/?dl=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AYO7STWJMP99"
   },
   "source": [
    "# Supplementary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OLULYdWEMahY"
   },
   "source": [
    "Optionally, if you want to incorporate labels for instruments, verbs and targets into your modelling. You can use the following code to decompose a triplet id into its corresponding instrument, verb and target ids, respectively, and vice versa. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jed9QAAJMR9s"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries for this module\n",
    "\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sl-bjvx6Nnkt"
   },
   "outputs": [],
   "source": [
    "# Ignore this cell if you have already downloaded and extracted the dataset\n",
    "\n",
    "!wget -O CholecT50_sample.zip https://seafile.unistra.fr/f/ba1427a82ecc4ce18566/?dl=1\n",
    "!unzip -o CholecT50_sample.zip\n",
    "\n",
    "print(\"Dataset successfully extracted!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZFgmFXInNSYJ"
   },
   "outputs": [],
   "source": [
    "num_instrument = 6\n",
    "num_verb     = 10\n",
    "num_target   = 15\n",
    "num_triplet  = 100\n",
    "map_dict_url = './CholecT50_sample/dict/maps.txt'\n",
    "maps_dict    = np.genfromtxt(map_dict_url, dtype=int, comments='#', delimiter=',', skip_header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FM2haCZvNahP"
   },
   "outputs": [],
   "source": [
    "def map_selector(component='iv'):\n",
    "    choices = {'ivt':0, 'i':1, 'v':2, 't':3, 'iv':4, 'it':5, 'vt':6, } \n",
    "    return  choices.get(component, 0) \n",
    "\n",
    "def decompose(inputs, component='i'):\n",
    "    \"\"\" Extract the component labels from the triplets. E.g.: from a triplet ID vector[100], get the vector [6] for the used instruments (i), or the vector [15] for the target acted upon. \n",
    "        Args:\n",
    "            inputs: a 1D vector of dimension (n), where n = number of triplet classes;\n",
    "                    with values int(0 or 1) for target labels and float[0, 1] for predicted labels.\n",
    "            component: a string for the component to extract; \n",
    "                    (e.g.: i for instrument, v for verb, t for target, iv for instrument-verb pair, it for instrument-target pair and vt (unused) for verb-target pair)\n",
    "        Returns:\n",
    "            output: int or float sparse encoding 1D vector of dimension (n), where n = number of component's classes.\n",
    "    \"\"\"\n",
    "    key    = map_selector(component)\n",
    "    index  = sorted(np.unique(maps_dict[:,key]))\n",
    "    output = []\n",
    "    for idx in index:\n",
    "        same_class = [i for i,x in enumerate(maps_dict[:,key]) if x==idx]\n",
    "        y = np.max(inputs[same_class])\n",
    "        output.append( y )        \n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sXUKqYRwNuEX"
   },
   "outputs": [],
   "source": [
    "# Create dictionary mapping triplet ids, instrument ids, verb ids and target ids to readable label\n",
    "\n",
    "dict_path = './CholecT50_sample/dict/'\n",
    "\n",
    "with open(os.path.join(dict_path, 'triplet.txt'), 'r') as f:\n",
    "  triplet_info = f.readlines()\n",
    "  triplet_dict = {}\n",
    "  for l in triplet_info:\n",
    "    triplet_id, triplet_label = l.split(':')\n",
    "    triplet_dict[int(triplet_id)] = triplet_label.rstrip()\n",
    "\n",
    "with open(os.path.join(dict_path, 'instrument.txt'), 'r') as f:\n",
    "  instrument_info = f.readlines()\n",
    "  instrument_dict = {}\n",
    "  for l in instrument_info:\n",
    "    instrument_id, instrument_label = l.split(':')\n",
    "    instrument_dict[int(instrument_id)] = instrument_label.rstrip()\n",
    "\n",
    "with open(os.path.join(dict_path, 'verb.txt'), 'r') as f:\n",
    "  verb_info = f.readlines()\n",
    "  verb_dict = {}\n",
    "  for l in verb_info:\n",
    "    verb_id, verb_label = l.split(':')\n",
    "    verb_dict[int(verb_id)] = verb_label.rstrip()\n",
    "\n",
    "with open(os.path.join(dict_path, 'target.txt'), 'r') as f:\n",
    "  target_info = f.readlines()\n",
    "  target_dict = {}\n",
    "  for l in target_info:\n",
    "    target_id, target_label = l.split(':')\n",
    "    target_dict[int(target_id)] = target_label.rstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L8gu9R4qPcoS"
   },
   "outputs": [],
   "source": [
    "print('Random triplet id and its human readable label\\n')\n",
    "random_triplet_id = np.random.choice(list(triplet_dict.keys()))\n",
    "print('Triplet id: ', random_triplet_id, '\\nReadable label: ', triplet_dict[random_triplet_id])\n",
    "\n",
    "one_hot_triplet = np.zeros(100)\n",
    "one_hot_triplet[random_triplet_id] = 1\n",
    "\n",
    "instrument_id = np.where(decompose(one_hot_triplet, 'i'))[0][0]\n",
    "verb_id = np.where(decompose(one_hot_triplet, 'v'))[0][0]\n",
    "target_id = np.where(decompose(one_hot_triplet, 't'))[0][0]\n",
    "\n",
    "print('Instrument id: ', instrument_id, '\\nReadable label: ', instrument_dict[instrument_id])\n",
    "print('Verb id: ', verb_id, '\\nReadable label: ', verb_dict[verb_id])\n",
    "print('Target id: ', target_id, '\\nReadable label: ', target_dict[target_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pNlvHE48SpaI"
   },
   "source": [
    "Additionally, if you want to learn the triplet as a 3d matrix of interaction as done in [Nwoye C.I. et.al, Recognition of Instrument-Tissue Interactions in Endoscopic Videos via Action Triplets, MICCAI 2020](https://arxiv.org/abs/2007.05405), we provide code below to map the a vector of triplet ids to its corresponding 3D matrix and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cvaXJujCQIlb"
   },
   "outputs": [],
   "source": [
    "\n",
    "def project_1d_to_3d(inputs):\n",
    "    \"\"\" Convert triplets labels from 1D vector to 3D matrix of interaction.\n",
    "        Args:\n",
    "            inputs: a 1D vector of dimension (n), where n = number of triplet classes;\n",
    "            with values int(0 or 1) for target labels and float[0, 1] for predicted labels.\n",
    "        Returns:\n",
    "            output: int or float sparse encoding 3D matrix of dimension (nI, nV, nT);\n",
    "            where nI = number of instrument classes,  nV = number of verb classes,  nT = number of target classes\n",
    "    \"\"\"\n",
    "    d3    = np.zeros([num_instrument, num_verb, num_target], dtype=np.float32)      \n",
    "    for idx, val in enumerate(inputs):\n",
    "        d3[maps_dict[idx,1], maps_dict[idx,2], maps_dict[idx,3]] = val\n",
    "    return d3\n",
    "    \n",
    "def project_3d_to_1d(self, inputs):\n",
    "    \"\"\" Convert triplets labels from 3D vector to 1D matrix of interaction.\n",
    "        Args:\n",
    "            inputs: a 3D matrix of dimension (nI, nV, nT);\n",
    "            where nI = number of instrument classes,  nV = number of verb classes,  nT = number of target classes\n",
    "        Returns:\n",
    "            output: int or float sparse encoding 1D vector of dimension (n), where n = number of triplet classes;\n",
    "            with values int(0 or 1) for target labels and float[0, 1] for predicted labels.\n",
    "    \"\"\"\n",
    "    d1   = np.zeros([num_triplet], dtype=np.float32)      \n",
    "    for idx in range(num_triplet):\n",
    "        d1[idx] = inputs[maps_dict[idx,1], maps_dict[idx,2], maps_dict[idx,3]]\n",
    "    return d1"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Getting Started.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
